---
title: "Sampler Comparison Analysis Using Bayesian Inference - Simulated Data"
author: "A.J. Brown"
date: "`r Sys.Date()`"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, include=FALSE}
# load libraries
library(rethinking)
library(dplyr)
library(tidyr)
library(tidyverse)
library(plotly)
```

TODO:
- investigate logNormal dists to prevent negative NO2 output

### Tools
Tool for printing precis() results with key:
```{r}
# Print precis results with sampler type legend
print_precis_with_correct_legend <- function(model, sim_data) {
  # Extract the factor levels from the sampler types in the data
  sampler_types <- levels(as.factor(sim_data$S))
  
  # Print precis results
  precis_results <- precis(model, depth = 2)
  
  # Add a legend mapping sampler type indices to their correct names
  cat("\nLegend:\n")
  for (i in seq_along(sampler_types)) {
    cat(sprintf("  %d = %s\n", i, sampler_types[i]))
  }
  precis_results
}
```

Tool for importing and prepping real data:
```{r}
# load real data
# note that this script places the working directory in the 2_code folder, not the root of the project
d <- read.csv("../1_data/real_data_ecfix.csv")

# drop inflow rows
d <- d[d$event.type != 'Inflow',]
# we will impute the missing values for the GB, GBH, and ISCO samplers at these events, but you may drop them if you like here:
# d <- d[!d$event.count %in% c('Storm 1', 'Storm 2'),]

# drop first samples from ISCO method to test "plume" theory for sediment bias
# commented out by default
# d <- d[!(d$duplicate == FALSE & d$method.name == "ISCO"), ]
  
# standardize analytes
d <- d %>%
  group_by(analyte_abbr) %>%
  mutate(result_ctr = standardize(result), #z-score standardization
         result_sc = (result - min(result, na.rm = T))/(max(result, na.rm = T) - min(result, na.rm = T)), # min-max standardization
         result_mx = result/max(result, na.rm = T),
         C_obs = result) %>%
  ungroup()

# rename block from 1 and 2 to "Block1" and "Block2"
d$block <- factor(d$block, levels = c(1, 2), labels = c("Block1", "Block2"))

# rename sampler names 
#"Grab Sample", "Hourly Grab", "ISCO", "Low-Cost Sampler" to
#"GB", "GBH", "ISCO", "LCS"
d$method.name <- factor(d$method.name, 
                        levels = c("Grab Sample", 
                                   "Hourly Grab", 
                                   "ISCO", 
                                   "Low-Cost Sampler"
                                   ), 
                        labels = c("GB", 
                                   "GBH", 
                                   "ISCO", 
                                   "LCS"
                                   )
                        )
# Create key pairs for consistent mapping with sim data
analyte_mapping <- c(
  "NO3" = 1,
  "NO2" = 2,
  "TKN" = 3,
  "pH" = 4,
  "TP" = 5,
  "OP" = 6,
  "EC" = 7,
  "TSS" = 8,
  "TDS" = 9
)

sampler_mapping <- c(
  "LCS" = 1,
  "ISCO" = 2,
  "GB" = 3,
  "GBH" = 4
)

block_mapping <- c(
  "Block1" = 1,
  "Block2" = 2
)

trt_mapping <- c(
  "CT" = 1,
  "MT" = 2,
  "ST" = 3
)

# Map keys pairs to values in new columns
d <- d %>%
  mutate(A = analyte_mapping[as.character(analyte_abbr)],
         S = sampler_mapping[as.character(method.name)],
         B = block_mapping[as.character(block)],
         TRT = trt_mapping[as.character(treatment)]
         )


# our model uses all data at once, no need to split
# Prepare data for the updated model
dat <- list(
  C_obs = d$result_ctr,                      # standardized results
  S = d$S,                                   # Sampler type index (1-4)
  A = d$A,                                   # Analyte index (1-9)
  B = d$B,                                   # Block index (1-2)
  TRT = d$TRT,                               # Treatment index (1-3)
  N = nrow(d),                               # Number of observations
  K_S = length(unique(d$S)),                 # Number of sampler types
  K_A = length(unique(d$analyte_abbr)),      # Number of analytes
  K_B = length(unique(d$block)),             # Number of blocks
  K_T = length(unique(d$treatment))          # Number of treatments
)
# Create observed means for each analyte and sampler type
observed_means <- d %>%
  group_by(
    analyte_type = factor(A, labels = names(analyte_mapping)), 
    sampler_type = factor(S, labels = names(sampler_mapping))
  ) %>%
  summarise(observed_mean = mean(C_obs, na.rm = TRUE), .groups = "drop")

# Generate summary statistics
summary_stats <- d %>%
  group_by(analyte_abbr) %>%
  summarise(
    mean_value = mean(result, na.rm = TRUE),
    median_value = median(result, na.rm = TRUE),
    sd_value = sd(result, na.rm = TRUE),
    min_value = min(result, na.rm = TRUE),
    max_value = max(result, na.rm = TRUE),
    count = n()
  ) %>%
  arrange(analyte_abbr)

# Output summary statistics
print(summary_stats)

# Save summary statistics to a CSV file
write.csv(summary_stats, "../1_data/real_data_summary_stats.csv", row.names = FALSE)

# Count samples collected:
d <- d %>%
  mutate(year = year(mdy_hm(collected)))  # if times are included

actual_composite_counts <- d %>%
  distinct(year, event.count, treatment, block, method.name, duplicate) %>%
  count(year, event.count, name = "actual_samples") %>%
  arrange(year, event.count)

print(actual_composite_counts)

# Save actual composite counts to a CSV file
write.csv(actual_composite_counts, "../1_data/real_data_composite_counts.csv", row.names = FALSE)
```

### Model 1.0 - Intercept Only
Data simulation
```{r, eval=FALSE}
simulate_model_1.0 <- function(n_per_type = 50, noise_sd = 0.2, seed = 42) {
  # Define sampler types and their true intercepts
  sampler_types <- c("LCS", "ISCO", "GB", "GBH")
  true_intercepts <- c(LCS = 0.7, ISCO = 0.3, GB = 1.2, GBH = 1.0)
  
  # Set seed for reproducibility
  set.seed(seed)
  
  # Simulate data
  sim_data <- data.frame(
    S = rep(sampler_types, each = n_per_type), # Sampler type
    C_obs = unlist(lapply(sampler_types, function(s) {
      rnorm(n_per_type, mean = true_intercepts[s], sd = noise_sd) # Add measurement noise
    }))
  )
  
  return(sim_data)
}

# Generate simulated data
sim_data_1.0 <- simulate_model_1.0()

```

Model fit
```{r, eval=FALSE}
# fit model 1.0, simple model with no partial pooling
# sim data for this model making the intercept coef have the following values:
# LCS should be 0.7, ISCO should be 0.3, GB should be 1.2 and GBH should be 1.0

# Prepare the data
data1.0 <- list(
  C_obs = sim_data_1.0$C_obs,                    # Observed concentrations
  S = as.numeric(as.factor(sim_data_1.0$S)),     # Sampler types as integers
  N = nrow(sim_data_1.0),                        # Number of observations
  K = length(unique(sim_data_1.0$S))             # Number of sampler types
)

# Fit the model
m1.0 <- ulam(
  alist(
    # Model for observed results
    C_obs ~ dnorm(mu, sigma),
    mu <- a[S],                  # Intercept for sampler type
    a[S] ~ dnorm(0, 0.5),        # Prior for intercepts
    sigma ~ dexp(1)              # Prior for measurement error
  ),
  data = data1.0,
  chains = 4,
  cores = 4
)

```

Summary of results
```{r, eval=FALSE}
# Use the function to display results
print_precis_with_correct_legend(m1.0, sim_data_1.0)
```

Result: The model is able to recover the true intercepts for each sampler type.

### Model 1.1 - Intercept Only with single analyte avg
Data Simulation
```{r, eval=FALSE}
simulate_model_1.1 <- function(n_per_type = 50, noise_sd = 0.2, seed = 42, avg_conc = 8) {
  # Define sampler types and their true intercepts
  sampler_types <- c("LCS", "ISCO", "GB", "GBH")
  true_intercepts <- c(LCS = 0.7, ISCO = 0.3, GB = 1.2, GBH = 1.0)
  
  # Set seed for reproducibility
  set.seed(seed)
  
  # Simulate data
  sim_data <- data.frame(
    S = rep(sampler_types, each = n_per_type), # Sampler type
    C_obs = unlist(lapply(sampler_types, function(s) {
      # Observed concentration = analyte average * sampler intercept + noise
      rnorm(n_per_type, mean = true_intercepts[s] * avg_conc, sd = noise_sd)
    }))
  )
  
  # Standardize observed concentrations
  sim_data$C_obs_standardized <- scale(sim_data$C_obs)
  
  # Calculate the z-score equivalents of the true intercepts
  z_scores <- (true_intercepts * avg_conc - mean(sim_data$C_obs)) / sd(sim_data$C_obs)
  
  # Print out the z-score equivalents for verification
  cat("Z-Score Equivalents of True Intercepts:\n")
  for (i in seq_along(sampler_types)) {
    cat(sprintf("  %s: %.3f\n", sampler_types[i], z_scores[i]))
  }
  
  return(sim_data)
}

# Generate simulated data
sim_data_1.1 <- simulate_model_1.1(avg_conc = 8)

```

Model Fit
```{r, eval=FALSE}
# fit model 1.1
# Prepare the data
data1.1 <- list(
  C_obs = standardize(sim_data_1.1$C_obs),       # Standardized Obs concentrations
  S = as.numeric(as.factor(sim_data_1.1$S)),     # Sampler types as integers
  N = nrow(sim_data_1.1),                        # Number of observations
  K = length(unique(sim_data_1.1$S))             # Number of sampler types
)

# Fit Model 1.1
m1.1 <- ulam(
  alist(
    # Model for observed results
    C_obs ~ dnorm(mu, sigma),
    mu <- a[S],                  # Intercept for sampler type
    a[S] ~ dnorm(0, 0.5),        # Prior for intercepts
    sigma ~ dexp(1)              # Prior for measurement error
  ),
  data = data1.1,
  chains = 4,
  cores = 4,
)
```

Summary of results with precis()
```{r, eval=FALSE}
# Use the function to display results
print_precis_with_correct_legend(m1.1, sim_data_1.1)
```

Results: Model 1.1 outputted expected z-scores for the true intercepts of the sampler types. The sampler effect intercepts were correctly estimated, with a correct std. deviation of 0.2.

### Model 1.2 - Multilevel model with sampler and analyte effects
Data simulation 
```{r, eval=FALSE}
simulate_model_1.2 <- function(n_per_type = 100, noise_sd = 0.2, seed = 45) {
  # Define sampler types and their true intercepts
  sampler_types <- c("LCS", "ISCO", "GB", "GBH")
  true_intercepts <- c(LCS = 0.7, ISCO = 0.3, GB = 1.2, GBH = 1.0) # Multiplicative effects
  
  # Define analytes' base means and standard deviations
  base_means <- c(NO3 = 8, NO2 = 0.1, TKN = 5, pH = 7, TP = 0.8, OP = 0.3, EC = 0.15, TSS = 1000, TDS = 500)
  base_sd <- c(NO3 = 1, NO2 = 0.01, TKN = 1, pH = 0.1, TP = 0.1, OP = 0.05, EC = 0.01, TSS = 200, TDS = 50)
  
  # Set seed for reproducibility
  set.seed(seed)
  
  # Create all combinations of analytes and sampler types
  sim_data <- expand.grid(
    analyte_abbr = seq_along(base_means), # Numeric index for analytes
    S = sampler_types,
    replicate = seq_len(n_per_type)       # Number of replicates per sampler type
  )
  
  # Map analyte_abbr back to analyte names for clarity
  analyte_map <- names(base_means)
  sim_data$analyte_name <- analyte_map[sim_data$analyte_abbr]
  
  # Generate observed concentrations
  sim_data$C_obs <- mapply(function(analyte, sampler) {
    mean_conc <- base_means[analyte]
    sd_conc <- base_sd[analyte]
    sampler_effect <- true_intercepts[sampler]
    rnorm(1, mean = mean_conc * sampler_effect, sd = sd_conc)
  }, analyte = sim_data$analyte_name, sampler = sim_data$S)
  
  # Standardize observed concentrations by analyte
  sim_data <- sim_data %>%
    group_by(analyte_abbr) %>%
    mutate(
      C_obs_standardized = scale(C_obs), # Standardized concentrations
      mean_obs = mean(C_obs),           # Mean of observed concentrations
      sd_obs = sd(C_obs)                # SD of observed concentrations
    ) %>%
    ungroup()
  
  # Calculate z-score equivalents of true intercepts for each analyte
  z_scores <- matrix(NA, nrow = length(base_means), ncol = length(sampler_types))
  for (idx in seq_along(base_means)) {
    analyte <- names(base_means)[idx]
    mean_conc <- base_means[analyte]
    sd_conc <- base_sd[analyte]
    z_scores[idx, ] <- sapply(sampler_types, function(sampler) {
      true_mean <- mean_conc * true_intercepts[sampler]
      z_score <- (true_mean - mean(sim_data$C_obs[sim_data$analyte_abbr == idx])) / 
                 sd(sim_data$C_obs[sim_data$analyte_abbr == idx])
      return(z_score)
    })
  }
  
  # Assign proper row and column names to z_scores
  dimnames(z_scores) <- list(names(base_means), sampler_types)
  
  # Print z-score equivalents
  cat("Z-Score Equivalents of True Intercepts by Analyte:\n")
  print(z_scores)
  
  # Drop unnecessary replicate column and keep only relevant columns
  sim_data <- sim_data %>% select(analyte_abbr, S, C_obs, C_obs_standardized)
  
  return(sim_data)
}

# Generate simulated data for Model 1.2
sim_data_1.2 <- simulate_model_1.2()

# print averages for C_obs per analyte_abbr stratified by sampler type as separate columns
sim_data_1.2 %>%
  group_by(analyte_abbr, S) %>%
  summarize(mean_C_obs = mean(C_obs), .groups = "drop") %>%
  spread(key = S, value = mean_C_obs) %>%
  print(n = Inf)

```

Model fit
```{r, eval=FALSE}
# Define data for Model 1.2
data1.2 <- list(
  C_obs = sim_data_1.2$C_obs_standardized,      # Standardized observed concentrations
  S = as.numeric(as.factor(sim_data_1.2$S)),    # Sampler type index (1-4)
  A = sim_data_1.2$analyte_abbr,                # Analyte index (1-9)
  N = nrow(sim_data_1.2),                       # Number of observations
  K_S = length(unique(sim_data_1.2$S)),         # Number of sampler types
  K_A = length(unique(sim_data_1.2$analyte_abbr)) # Number of analytes
)

# Fit Model 1.2
m1.2 <- ulam(
  alist(
    # Observation model
    C_obs ~ dnorm(mu, sigma),
    
    # Mean structure: interaction between analyte and sampler type
    mu <- alpha[A] + beta[A, S],
    
    # Priors for analyte-specific intercepts
    matrix[A, S]:beta ~ dnorm(0, 0.5),
    alpha[A] ~ dnorm(a_bar, sigma_a),
    
    # Hyper-priors
    a_bar ~ dnorm(0, 0.5),
    sigma_a ~ dexp(1),
    
    # Prior for measurement error
    sigma ~ dexp(1)
  ),
  data = data1.2,
  chains = 4,
  cores = 4
)

```

Summary of results 
```{r, eval=FALSE}
# Use the function to display results
print_precis_with_correct_legend(m1.2, sim_data_1.2)
```

Posterior predictions
```{r, eval=FALSE}
# Extract the posterior samples
post1.2 <- extract.samples(m1.2)

# Create a link function
link_function <- function(post, data) {
  # Extract necessary dimensions
  n_samples <- nrow(post$beta[, , 1])  # Number of posterior samples
  n_analytes <- data$K_A               # Number of analytes
  n_samplers <- data$K_S               # Number of sampler types
  
  # Initialize matrix to store predictions
  predictions <- array(NA, dim = c(n_samples, n_analytes, n_samplers))
  
  # Generate predictions for each analyte-sampler combination
  for (a in 1:n_analytes) {
    for (s in 1:n_samplers) {
      predictions[, a, s] <- post$alpha[, a] + post$beta[, a, s]
    }
  }
  
  return(predictions)
}

# Apply the link function to generate posterior predictions
posterior_predictions <- link_function(post1.2, data1.2)

# Back-transform predictions
back_transformed_predictions_1.2 <- array(NA, dim = dim(posterior_predictions))

for (a in 1:data1.2$K_A) {
  # Extract mean and SD for the analyte
  analyte_mean <- mean(sim_data_1.2$C_obs[sim_data_1.2$analyte_abbr == a])
  analyte_sd <- sd(sim_data_1.2$C_obs[sim_data_1.2$analyte_abbr == a])
  
  # Back-transform predictions for the analyte
  back_transformed_predictions_1.2[, a, ] <- posterior_predictions[, a, ] * analyte_sd + analyte_mean
}

# Summarize predictions
prediction_summary <- data.frame()

for (a in 1:data1.2$K_A) {
  for (s in 1:data1.2$K_S) {
    # Extract posterior samples for this analyte-sampler combination
    samples <- back_transformed_predictions_1.2[, a, s]
    
    # Compute summary statistics
    summary_row <- data.frame(
      analyte_abbr = a,
      sampler_type = levels(sim_data_1.2$S)[s],
      mean_prediction = mean(samples),
      prediction_5.5 = quantile(samples, 0.055),
      prediction_94.5 = quantile(samples, 0.945)
    )
    
    prediction_summary <- rbind(prediction_summary, summary_row)
  }
}

# View summarized predictions
# Refine the summary table: Rows as analytes, columns as sampler types
refined_summary_table <- prediction_summary %>%
  select(analyte_abbr, sampler_type, mean_prediction) %>%
  pivot_wider(names_from = sampler_type, values_from = mean_prediction)

# View the refined summary table
refined_summary_table
```

Frequentist Model Equivalent for Sanity Check
```{r, eval=FALSE}
# frequentist model to verify model form
# Fit the linear model
lm_fit <- lm(C_obs ~ as.factor(analyte_abbr) + as.factor(analyte_abbr):S, data = sim_data_1.2)

# Summarize the model
summary(lm_fit)

# Generate predictions using the frequentist linear model
sim_data_1.2$lm_predicted <- predict(lm_fit)

# Summarize predictions into a table
lm_summary_table <- sim_data_1.2 %>%
  group_by(analyte_abbr, S) %>%
  summarize(mean_prediction = mean(lm_predicted), .groups = "drop") %>%
  pivot_wider(names_from = S, values_from = mean_prediction)

lm_summary_table

```

Results: Model 1.2 was successful in estimating analyte and sampler effects in a single model with varying standard deviations correctly.

### Model 1.3 - Multilevel model with sampler, analyte, and block effects
Data simulation
```{r}
simulate_model_1.3 <- function(n_per_type = 100, noise_sd = 0.2, seed = 45) {
  # Define sampler types and their true intercepts
  sampler_types <- c("LCS", "ISCO", "GB", "GBH")
  true_intercepts <- c(LCS = 0.7, ISCO = 0.3, GB = 1.2, GBH = 1.0)
  
  # Define analytes' base means and standard deviations
  base_means <- c(NO3 = 8, NO2 = 0.1, TKN = 5, pH = 7, TP = 0.8, OP = 0.3, EC = 0.15, TSS = 1000, TDS = 500)
  base_sd <- c(NO3 = 1, NO2 = 0.01, TKN = 1, pH = 0.1, TP = 0.1, OP = 0.05, EC = 0.01, TSS = 200, TDS = 50)
  
  # Define block-specific effects
  #block_effects <- c(Block1 = 0, Block2 = 0.5)
  block_effects <- c(Block1 = 1, Block2 = 0.8)
  
  # Set seed for reproducibility
  set.seed(seed)
  
  # Create all combinations of analytes, sampler types, and blocks
  sim_data <- expand.grid(
    analyte_abbr = seq_along(base_means), # Numeric index for analytes
    S = sampler_types,
    block = names(block_effects),
    replicate = seq_len(n_per_type / 2)
  )
  
  # Map analyte_abbr to analyte_name
  analyte_map <- names(base_means)
  sim_data$analyte_name <- analyte_map[sim_data$analyte_abbr]
  
  # Generate observed concentrations
  sim_data <- sim_data %>%
    rowwise() %>%
    mutate(
      C_obs = rnorm(
        1,
        # mean = base_means[analyte_name] * true_intercepts[S] + block_effects[block] * base_sd[analyte_name],
        mean = base_means[analyte_name] +
          (-base_means[analyte_name]*(1-true_intercepts[S])) +
          (-base_means[analyte_name]*(1-block_effects[block])),
        sd = base_sd[analyte_name]
      )
    ) %>%
    ungroup()
  
  # Standardize observed concentrations by analyte
  sim_data <- sim_data %>%
    group_by(analyte_abbr) %>%
    mutate(C_obs_standardized = scale(C_obs)) %>%
    ungroup()
  
  # Select relevant columns
  sim_data <- sim_data %>% select(analyte_abbr, analyte_name, S, block, C_obs, C_obs_standardized)
  
  return(sim_data)
}

# Generate standardized simulated data for Model 1.3
sim_data_1.3 <- simulate_model_1.3()

# Summarize the averages of C_obs per analyte_abbr stratified by sampler type and block
sim_data_1.3 %>%
  group_by(analyte_abbr, analyte_name, S, block) %>%
  summarize(
    mean_C_obs = mean(C_obs),
    .groups = "drop"
  ) %>%
  tidyr::spread(key = S, value = mean_C_obs) %>%
  print(n = Inf)

```

Model fit - TEST DATA
```{r, eval=FALSE}
# Prepare data for the updated model
data1.3 <- list(
  C_obs = sim_data_1.3$C_obs_standardized,  # Standardized observed concentrations
  S = as.numeric(as.factor(sim_data_1.3$S)), # Sampler type index (1-4)
  A = sim_data_1.3$analyte_abbr,             # Analyte index (1-9)
  B = as.numeric(as.factor(sim_data_1.3$block)), # Block index (1-2)
  N = nrow(sim_data_1.3),                    # Number of observations
  K_S = length(unique(sim_data_1.3$S)),      # Number of sampler types
  K_A = length(unique(sim_data_1.3$analyte_abbr)), # Number of analytes
  K_B = length(unique(sim_data_1.3$block))   # Number of blocks
)

# Updated model allowing block effects to vary by analyte
m1.3 <- ulam(
  alist(
    # Observation model
    C_obs ~ dnorm(mu, sigma),
    
    # Mean structure with analyte-specific block effects
    mu <- alpha[A] + beta[A, S] + gamma[A, B],
    
    # Non-centered parameterization for analyte-specific intercepts
    transpars> vector[K_A]:alpha <<- a_bar + z_alpha * sigma_a,
    vector[K_A]:z_alpha ~ normal(0, 1),
    
    # Non-centered parameterization for analyte-specific block effects
    transpars> matrix[K_A, K_B]:gamma <<- g_bar + z_gamma * sigma_g,
    matrix[K_A, K_B]:z_gamma ~ normal(0, 1),
    
    # Priors for sampler-specific effects
    matrix[K_A, K_S]:beta ~ normal(0, 1),
    
    # Hyper-priors
    a_bar ~ normal(0, 1),
    sigma_a ~ exponential(1),
    g_bar ~ normal(0, 0.5),
    sigma_g ~ exponential(1),
    
    # Prior for measurement error
    sigma ~ exponential(1)
  ),
  data = data1.3,
  chains = 4,
  cores = 4
)

```

Summary of results
```{r, eval=FALSE}
precis(m1.3, depth=2)
```

```{r, eval=FALSE}
traceplot(m1.3)
```

Posterior predictions
```{r, eval=FALSE}
# Extract posterior samples
post1.3 <- extract.samples(m1.3)

# Updated link function for posterior predictions
link_function <- function(post1.3, data) {
  n_samples <- nrow(post1.3$beta[, , 1])  # Number of posterior samples
  n_analytes <- data$K_A               # Number of analytes
  n_samplers <- data$K_S               # Number of sampler types
  n_blocks <- data$K_B                 # Number of blocks
  
  # Initialize predictions array
  predictions <- array(NA, dim = c(n_samples, n_analytes, n_samplers, n_blocks))
  
  # Generate predictions
  for (a in 1:n_analytes) {
    for (s in 1:n_samplers) {
      for (b in 1:n_blocks) {
        # Compute predictions in z-score form
        predictions[, a, s, b] <- post1.3$a_bar +
                                  post1.3$z_alpha[, a] * post1.3$sigma_a +
                                  post1.3$beta[, a, s] +
                                  post1.3$g_bar +
                                  post1.3$z_gamma[, a, b] * post1.3$sigma_g
      }
    }
  }
  
  return(predictions)
}

# Generate predictions
posterior_predictions <- link_function(post1.3, data1.3)

# Initialize array for back-transformed predictions
back_transformed_predictions_1.3 <- array(NA, dim = dim(posterior_predictions))

# Back-transform predictions for each analyte
for (a in 1:data1.3$K_A) {
  # Extract mean and SD for the analyte
  analyte_mean <- mean(sim_data_1.3$C_obs[sim_data_1.3$analyte_abbr == a])
  analyte_sd <- sd(sim_data_1.3$C_obs[sim_data_1.3$analyte_abbr == a])
  
  # Back-transform predictions for the analyte
  back_transformed_predictions_1.3[, a, , ] <- posterior_predictions[, a, , ] * analyte_sd + analyte_mean
}


# Summarize posterior predictions
# Initialize a summary dataframe
prediction_summary <- data.frame()

# Summarize predictions for each analyte, sampler, and block
for (a in 1:data1.3$K_A) {
  for (s in 1:data1.3$K_S) {
    for (b in 1:data1.3$K_B) {
      # Extract samples for the current analyte, sampler, and block
      samples <- back_transformed_predictions1.3[, a, s, b]
      
      # Add summary statistics to the dataframe
      summary_row <- data.frame(
        analyte_abbr = a,
        sampler_type = as.character(levels(sim_data_1.3$S)[s]),
        block = as.character(levels(sim_data_1.3$block)[b]),
        mean_prediction = mean(samples),
        prediction_5.5 = quantile(samples, 0.055),
        prediction_94.5 = quantile(samples, 0.945)
      )
      
      prediction_summary <- rbind(prediction_summary, summary_row)
    }
  }
}

# Refine the summary table: Rows as analytes, columns as sampler types
refined_summary_table_1.3 <- prediction_summary %>%
  select(analyte_abbr, sampler_type, mean_prediction, block) %>%
  pivot_wider(names_from = sampler_type, values_from = mean_prediction)

# View the refined summary table
refined_summary_table_1.3
```

Results: Model 1.3 now works as expected, with block effects varying by analyte. The refined summary table shows the mean predictions for each analyte, sampler type, and block. The model can be further refined by incorporating multilevel structures for characterizing correlations between the block effects, sampler effects, and analyte-specific intercepts. 

### Model 1.4 - Multilevel model with sampler, analyte, and block effects with MVN distribution between analytes, samplers, and blocks

**In Development**

Data simulation
```{r, eval=FALSE}
#TBD if we want to try simulating correlations between parameters
sim_data_1.4 <- simulate_model_1.3()
```

#### Centered Model
- Model fit
```{r, eval=FALSE}
# Prepare data for the model
data1.4 <- list(
  C_obs = sim_data_1.4$C_obs_standardized,  # Standardized observed concentrations
  S = as.numeric(as.factor(sim_data_1.4$S)), # Sampler type index (1-4)
  A = sim_data_1.4$analyte_abbr,             # Analyte index (1-9)
  B = as.numeric(as.factor(sim_data_1.4$block)), # Block index (1-2)
  N = nrow(sim_data_1.4),                    # Number of observations
  K_S = length(unique(sim_data_1.4$S)),      # Number of sampler types
  K_A = length(unique(sim_data_1.4$analyte_abbr)), # Number of analytes
  K_B = length(unique(sim_data_1.4$block))   # Number of blocks
)

# Non-centered version of m1.4 (corrected)
m1.4_c <- ulam(
  alist(
    # Observation model
    C_obs ~ dnorm(mu, sigma),
    
    # Mean structure with correlations between analytes, samplers, and blocks
    mu <- alpha[A] + beta[A, S] + gamma[A, B], #these S, A, and B vars need to BE IN THIS ORDER
    
    # Analyte-specific (9 levels) intercepts 
    alpha[A] ~ dnorm(0, 1),
    
    # Analyte-specific (9 levels) sampler (4 levels) effects (centered parameterization) with covariance
    vector[K_S]:beta[A] ~ multi_normal(0, Rho_sampler, sigma_sampler),
    vector[K_B]:gamma[A] ~ multi_normal(0, Rho_block, sigma_block),
    
    # Fixed priors for sampler and block effects
    sigma_sampler ~ dexp(1),  # Prior for scaling sampler effects
    Rho_sampler ~ dlkjcorr(4),  # LKJ prior for sampler effects
    sigma_block ~ dexp(1),  # Prior for scaling block effects
    Rho_block ~ dlkjcorr(4),  # LKJ prior for block effects
    
    # Prior for measurement error
    sigma ~ dexp(1)
  ),
  data = data1.4,
  chains = 4,
  cores = 12, # reduce to 4 for potato computers
  iter = 2000, # total iterations (increase this to try and get better convergence)
  warmup = 1000
)
```

- Summary of results
```{r, eval=FALSE}
precis(m1.4_c, depth = 2)
```

- Posterior Predictions
```{r, eval=FALSE}
# Extract posterior samples
post1.4_c <- extract.samples(m1.4_c)

# Link function for posterior predictions (Centered Model)
link_function_1.4_c <- function(post, data) {
  n_samples <- nrow(post$alpha)      # Number of posterior samples
  n_analytes <- data$K_A            # Number of analytes
  n_samplers <- data$K_S            # Number of sampler types
  n_blocks <- data$K_B              # Number of blocks

  # Initialize an array to store predictions
  predictions <- array(NA, dim = c(n_samples, n_analytes, n_samplers, n_blocks))

  # Generate predictions for each analyte, sampler, and block
  for (a in 1:n_analytes) {
    for (s in 1:n_samplers) {
      for (b in 1:n_blocks) {
        predictions[, a, s, b] <- post$alpha[, a] +
                                  post$beta[, a, s] +   # sampler effect for analyte a
                                  post$gamma[, a, b]    # block effect for analyte a
      }
    }
  }

  return(predictions)
}


# Generate predictions for the centered model
posterior_predictions_1.4_c <- link_function_1.4_c(post1.4_c, data1.4)

# Initialize an array for back-transformed predictions
back_transformed_predictions_1.4_c <- posterior_predictions_1.4_c

# Back-transform predictions for each analyte
for (a in 1:data1.4$K_A) {
  analyte_mean <- mean(sim_data_1.4$C_obs[sim_data_1.4$analyte_abbr == a])
  analyte_sd <- sd(sim_data_1.4$C_obs[sim_data_1.4$analyte_abbr == a])

  # Apply back-transformation
  back_transformed_predictions_1.4_c[, a, , ] <- 
    posterior_predictions_1.4_c[, a, , ] * analyte_sd + analyte_mean
}

# Summarize posterior predictions
prediction_summary_1.4_c <- data.frame()

for (a in 1:data1.4$K_A) {
  for (s in 1:data1.4$K_S) {
    for (b in 1:data1.4$K_B) {
      samples <- back_transformed_predictions_1.4_c[, a, s, b]
      summary_row <- data.frame(
        analyte_abbr = a,
        sampler_type = s,
        block = b,
        mean_prediction = mean(samples),
        prediction_5.5 = quantile(samples, 0.055),
        prediction_94.5 = quantile(samples, 0.945)
      )
      prediction_summary_1.4_c <- rbind(prediction_summary_1.4_c, summary_row)
    }
  }
}

# Refine the summary table
refined_summary_table_1.4_c <- prediction_summary_1.4_c %>%
  select(analyte_abbr, sampler_type, mean_prediction, block) %>%
  pivot_wider(names_from = sampler_type, values_from = mean_prediction)

refined_summary_table_1.4_c

```


#### Non-centered model
- Model fit
```{r, eval=TRUE}
# Non-centered version of m1.4
# Prepare data for the model
data1.4 <- list(
  C_obs = sim_data_1.4$C_obs_standardized,  # Standardized observed concentrations
  S = as.numeric(as.factor(sim_data_1.4$S)), # Sampler type index (1-4)
  A = sim_data_1.4$analyte_abbr,             # Analyte index (1-9)
  B = as.numeric(as.factor(sim_data_1.4$block)), # Block index (1-2)
  N = nrow(sim_data_1.4),                    # Number of observations
  K_S = length(unique(sim_data_1.4$S)),      # Number of sampler types
  K_A = length(unique(sim_data_1.4$analyte_abbr)), # Number of analytes
  K_B = length(unique(sim_data_1.4$block))   # Number of blocks
)

# Non-centered version of m1.4 (corrected)
m1.4_nc <- ulam(
  alist(
    # Observation model
    C_obs ~ dnorm(mu, sigma),
    
    # Mean structure with correlations between analytes, samplers, and blocks
    mu <- alpha[A] + beta[A, S] + gamma[A, B],
    
    # Analyte-specific (9 levels) intercepts (non-centered parameterization)
    transpars> vector[K_A]:alpha <<- mu_alpha + z_alpha * sigma_alpha,
    vector[K_A]:z_alpha ~ normal(0, 1),  # Non-centered scaling
    mu_alpha ~ normal(0, 1),  # Prior for mean intercepts
    sigma_alpha ~ exponential(1),  # Prior for intercept variation
    
    # Analyte-specific (9 levels) sampler (4 levels) effects (non-centered parameterization)
    transpars> matrix[K_A, K_S]:beta <<- mu_beta + z_beta * diag_pre_multiply(sigma_beta, L_beta),
    transpars> matrix[K_A, K_S]:mu_beta <- rep_matrix(0, K_A, K_S),  # Center at zero
    matrix[K_A, K_S]:z_beta ~ normal(0, 1),  # Standardized effects
    transpars> cholesky_factor_corr[K_S]:L_beta ~ lkj_corr_cholesky(2),  # LKJ prior
    vector[K_S]:sigma_beta ~ exponential(1),  # Prior for scaling sampler effects
    
    # Analyte-specific (9 levels) block (2 levels) effects (non-centered parameterization)
    transpars> matrix[K_A, K_B]:gamma <<- mu_gamma + z_gamma * diag_pre_multiply(sigma_gamma, L_gamma),
    transpars> matrix[K_A, K_B]:mu_gamma <- rep_matrix(0, K_A, K_B),  # Center at zero
    matrix[K_A, K_B]:z_gamma ~ normal(0, 1),  # Standardized effects
    transpars> cholesky_factor_corr[K_B]:L_gamma ~ lkj_corr_cholesky(2),  # LKJ prior
    vector[K_B]:sigma_gamma ~ exponential(1),  # Prior for scaling block effects
    
    # Prior for measurement error
    sigma ~ exponential(1)
  ),
  data = data1.4,
  chains = 4,
  cores = 12,
  iter = 2000,              # Total iterations (increase this)
  warmup = 1000             # Number of warmup iterations (optional, default is iter/2)
)

```

- Summary of results
```{r, eval=TRUE}
precis(m1.4_nc, depth = 2)
```

- Posterior predictions
```{r, eval=TRUE}
# Extract posterior samples
post1.4_nc <- extract.samples(m1.4_nc)

# Updated link function for posterior predictions
link_function_1.4_nc <- function(post1.4_nc, data) {
  n_samples <- nrow(post1.4_nc$alpha)       # Number of posterior samples
  n_analytes <- data$K_A                # Number of analytes
  n_samplers <- data$K_S                # Number of sampler types
  n_blocks <- data$K_B                  # Number of blocks

  # Initialize predictions array
  predictions_nc <- array(NA, dim = c(n_samples, n_analytes, n_samplers, n_blocks))

  # Generate predictions
  for (a in 1:n_analytes) {
    for (s in 1:n_samplers) {
      for (b in 1:n_blocks) {
        # Compute predictions using the new model structure
        predictions_nc[, a, s, b] <- post1.4_nc$mu_alpha +
                                  post1.4_nc$z_alpha[, a] * post1.4_nc$sigma_alpha +
                                  post1.4_nc$beta[, a, s] +
                                  post1.4_nc$gamma[, a, b]
      }
    }
  }

  return(predictions_nc)
}

# Generate predictions
posterior_predictions_1.4_nc <- link_function_1.4_nc(post1.4_nc, data1.4)

# Initialize array for back-transformed predictions
back_transformed_predictions_1.4_nc <- array(NA, dim = dim(posterior_predictions_1.4_nc))

# Back-transform predictions for each analyte
for (a in 1:data1.4$K_A) {
  # Extract mean and SD for the analyte from `sim_data_1.4`
  analyte_mean <- mean(sim_data_1.4$C_obs[sim_data_1.4$analyte_abbr == a])
  analyte_sd <- sd(sim_data_1.4$C_obs[sim_data_1.4$analyte_abbr == a])

  # Back-transform predictions for the analyte
  back_transformed_predictions_1.4_nc[, a, , ] <- posterior_predictions_1.4_nc[, a, , ] * analyte_sd + analyte_mean
}

# Summarize posterior predictions
# Initialize a summary dataframe
prediction_summary_1.4_nc <- data.frame()

# Summarize predictions for each analyte, sampler, and block
for (a in 1:data1.4$K_A) {
  for (s in 1:data1.4$K_S) {
    for (b in 1:data1.4$K_B) {
      # Extract samples for the current analyte, sampler, and block
      samples <- back_transformed_predictions_1.4_nc[, a, s, b]

      # Add summary statistics to the dataframe
      summary_row <- data.frame(
        analyte_abbr = a,
        sampler_type = s,
        block = b,
        mean_prediction = mean(samples),
        prediction_5.5 = quantile(samples, 0.055),
        prediction_94.5 = quantile(samples, 0.945)
      )

      prediction_summary_1.4_nc <- rbind(prediction_summary_1.4_nc, summary_row)
    }
  }
}

# Refine the summary table: Rows as analytes, columns as sampler types
refined_summary_table_1.4_nc <- prediction_summary_1.4_nc %>%
  select(analyte_abbr, sampler_type, mean_prediction, block) %>%
  pivot_wider(names_from = sampler_type, values_from = mean_prediction)

# View the refined summary table
refined_summary_table_1.4_nc

```

Results: The non-centered model performs better, and can accurately estimate simulated data.

### Model 1.5 - same as 1.4 but with different model form to mimic simulated data - NOT AS GOOD AS 1.4
Data simulation
```{r}
sim_data_1.5 <- simulate_model_1.3()
```

Model fit
```{r}
# Prepare data for the model
data1.5 <- list(
  C_obs = sim_data_1.5$C_obs_standardized,  # Standardized observed concentrations
  S = as.numeric(as.factor(sim_data_1.5$S)), # Sampler type index (1-4)
  A = sim_data_1.5$analyte_abbr,             # Analyte index (1-9)
  B = as.numeric(as.factor(sim_data_1.5$block)), # Block index (1-2)
  N = nrow(sim_data_1.5),                    # Number of observations
  K_S = length(unique(sim_data_1.5$S)),      # Number of sampler types
  K_A = length(unique(sim_data_1.5$analyte_abbr)), # Number of analytes
  K_B = length(unique(sim_data_1.5$block))   # Number of blocks
)

# Non-centered version of m1.5 with new model form
m1.5_nc <- ulam(
  alist(
    # Observation model
    C_obs ~ dnorm(mu, sigma),
    
    # Mean structure with correlations between analytes, samplers, and blocks
    mu <- alpha[A] - alpha[A] * (1 - beta[A, S]) - alpha[A] * (1 - gamma[A, B]),
    
    # Analyte-specific (9 levels) intercepts (non-centered parameterization)
    transpars> vector[K_A]:alpha <<- mu_alpha + z_alpha * sigma_alpha,
    vector[K_A]:z_alpha ~ normal(0, 1),  # Non-centered scaling
    mu_alpha ~ normal(0, 1),  # Prior for mean intercepts
    sigma_alpha ~ exponential(1),  # Prior for intercept variation
    
    # Analyte-specific (9 levels) sampler (4 levels) effects (non-centered parameterization)
    transpars> matrix[K_A, K_S]:beta <<- mu_beta + z_beta * diag_pre_multiply(sigma_beta, L_beta),
    transpars> matrix[K_A, K_S]:mu_beta <- rep_matrix(0, K_A, K_S),  # Center at zero
    matrix[K_A, K_S]:z_beta ~ normal(0, 1),  # Standardized effects
    transpars> cholesky_factor_corr[K_S]:L_beta ~ lkj_corr_cholesky(2),  # LKJ prior
    vector[K_S]:sigma_beta ~ exponential(1),  # Prior for scaling sampler effects
    
    # Analyte-specific (9 levels) block (2 levels) effects (non-centered parameterization)
    transpars> matrix[K_A, K_B]:gamma <<- mu_gamma + z_gamma * diag_pre_multiply(sigma_gamma, L_gamma),
    transpars> matrix[K_A, K_B]:mu_gamma <- rep_matrix(0, K_A, K_B),  # Center at zero
    matrix[K_A, K_B]:z_gamma ~ normal(0, 1),  # Standardized effects
    transpars> cholesky_factor_corr[K_B]:L_gamma ~ lkj_corr_cholesky(2),  # LKJ prior
    vector[K_B]:sigma_gamma ~ exponential(1),  # Prior for scaling block effects
    
    # Prior for measurement error
    sigma ~ exponential(1)
  ),
  data = data1.5,
  chains = 4,
  cores = 12,
  iter = 2000,              # Total iterations (increase this)
  warmup = 1000             # Number of warmup iterations (optional, default is iter/2)
)
```

Summary of results
```{r}
precis(m1.5_nc, depth = 2)
```

Posterior predictions
```{r}
# Extract posterior samples
post1.5_nc <- extract.samples(m1.5_nc)

# Updated link function for posterior predictions for m1.5_nc
link_function_1.5_nc <- function(post1.5_nc, data) {
  n_samples <- nrow(post1.5_nc$alpha)       # Number of posterior samples
  n_analytes <- data$K_A                   # Number of analytes
  n_samplers <- data$K_S                   # Number of sampler types
  n_blocks <- data$K_B                     # Number of blocks

  # Initialize predictions array
  predictions_nc <- array(NA, dim = c(n_samples, n_analytes, n_samplers, n_blocks))

  # Generate predictions
  for (a in 1:n_analytes) {
    for (s in 1:n_samplers) {
      for (b in 1:n_blocks) {
        # Compute predictions using the new mean structure
        predictions_nc[, a, s, b] <- post1.5_nc$mu_alpha +
                                     post1.5_nc$z_alpha[, a] * post1.5_nc$sigma_alpha -
                                     post1.5_nc$alpha[, a] * (1 - post1.5_nc$beta[, a, s]) -
                                     post1.5_nc$alpha[, a] * (1 - post1.5_nc$gamma[, a, b])
      }
    }
  }

  return(predictions_nc)
}

# Generate predictions
posterior_predictions_1.5_nc <- link_function_1.5_nc(post1.5_nc, data1.5)

# Initialize array for back-transformed predictions
back_transformed_predictions_1.5_nc <- array(NA, dim = dim(posterior_predictions_1.5_nc))

# Back-transform predictions for each analyte
for (a in 1:data1.5$K_A) {
  # Extract mean and SD for the analyte from `sim_data_1.5`
  analyte_mean <- mean(sim_data_1.5$C_obs[sim_data_1.5$analyte_abbr == a])
  analyte_sd <- sd(sim_data_1.5$C_obs[sim_data_1.5$analyte_abbr == a])

  # Back-transform predictions for the analyte
  back_transformed_predictions_1.5_nc[, a, , ] <- posterior_predictions_1.5_nc[, a, , ] * analyte_sd + analyte_mean
}

# Summarize posterior predictions
# Initialize a summary dataframe
prediction_summary_1.5_nc <- data.frame()

# Summarize predictions for each analyte, sampler, and block
for (a in 1:data1.5$K_A) {
  for (s in 1:data1.5$K_S) {
    for (b in 1:data1.5$K_B) {
      # Extract samples for the current analyte, sampler, and block
      samples <- back_transformed_predictions_1.5_nc[, a, s, b]

      # Add summary statistics to the dataframe
      summary_row <- data.frame(
        analyte_abbr = a,
        sampler_type = s,
        block = b,
        mean_prediction = mean(samples),
        prediction_5.5 = quantile(samples, 0.055),
        prediction_94.5 = quantile(samples, 0.945)
      )

      prediction_summary_1.5_nc <- rbind(prediction_summary_1.5_nc, summary_row)
    }
  }
}

# Refine the summary table: Rows as analytes, columns as sampler types
refined_summary_table_1.5_nc <- prediction_summary_1.5_nc %>%
  select(analyte_abbr, sampler_type, mean_prediction, block) %>%
  pivot_wider(names_from = sampler_type, values_from = mean_prediction)

# View the refined summary table
refined_summary_table_1.5_nc

```

Results:



### Model 1.6 - Multilevel model with sampler, analyte, TRT, and block effects with MVN distribution between analytes, samplers, treatments and blocks

Data simulation
```{r, eval=FALSE}
simulate_model_1.6 <- function(n_per_type = 100, noise_sd = 0.2, seed = 42) {
  # Define sampler types and their true intercepts
  sampler_types <- c("LCS", "ISCO", "GB", "GBH")
  true_intercepts <- c(LCS = 0.7, ISCO = 0.3, GB = 1.2, GBH = 1.0)
  
  # Define analytes' base means and standard deviations
  base_means <- c(NO3 = 8, NO2 = 0.1, TKN = 5, pH = 7, TP = 0.8, OP = 0.3, EC = 0.15, TSS = 1000, TDS = 500)
  base_sd <- c(NO3 = 1, NO2 = 0.01, TKN = 1, pH = 0.1, TP = 0.1, OP = 0.05, EC = 0.01, TSS = 200, TDS = 50)
  
  # Define block-specific effects
  block_effects <- c(Block1 = 1, Block2 = 0.8)
  
  # Define treatment-specific effects
  treatment_effects <- c(CT = 1.2, MT = 1, ST = 0.8)
  
  # Set seed for reproducibility
  set.seed(seed)
  
  # Create all combinations of analytes, sampler types, blocks, and treatments
  sim_data <- expand.grid(
    analyte_abbr = seq_along(base_means),  # Numeric index for analytes
    S = sampler_types,
    block = names(block_effects),
    treatment = names(treatment_effects),
    replicate = seq_len(n_per_type / 2)
  )
  
  # Map analyte_abbr to analyte_name
  analyte_map <- names(base_means)
  sim_data$analyte_name <- analyte_map[sim_data$analyte_abbr]
  
  # Generate observed concentrations
  sim_data <- sim_data %>%
    rowwise() %>%
    mutate(
      C_obs = rnorm(
        1,
        mean = base_means[analyte_name] +
          (-base_means[analyte_name] * (1 - true_intercepts[S])) +
          (-base_means[analyte_name] * (1 - block_effects[block])) +
          (-base_means[analyte_name] * (1 - treatment_effects[treatment])),
        sd = base_sd[analyte_name]
      )
    ) %>%
    ungroup()
  
  # Standardize observed concentrations by analyte
  sim_data <- sim_data %>%
    group_by(analyte_abbr) %>%
    mutate(C_obs_standardized = scale(C_obs)) %>%
    ungroup()
  
  # Select relevant columns
  sim_data <- sim_data %>% select(analyte_abbr, analyte_name, S, block, treatment, C_obs, C_obs_standardized)
  
  return(sim_data)
}

# Generate simulated data for Model 1.6
sim_data_1.6 <- simulate_model_1.6()

# Summarize the averages of C_obs per analyte_abbr, stratified by sampler type, block, and treatment
sim_data_1.6 %>%
  group_by(analyte_abbr, analyte_name, S, block, treatment) %>%
  summarize(mean_C_obs = mean(C_obs), .groups = "drop") %>%
  tidyr::spread(key = S, value = mean_C_obs) %>%
  print(n = Inf)
```

#### Non-centered model
- Model fit
```{r, eval=TRUE}
# Prepare data for the model
data1.6 <- list(
  C_obs = as.numeric(sim_data_1.6$C_obs_standardized),   # Standardized observed concentrations
  S = as.integer(as.factor(sim_data_1.6$S)),             # Sampler method
  A = as.integer(sim_data_1.6$analyte_abbr),             # Analyte type
  B = as.integer(as.factor(sim_data_1.6$block)),         # Block
  TRT = as.integer(as.factor(sim_data_1.6$treatment)),   # Tillage treatment
  N = nrow(sim_data_1.6),
  K_S = length(unique(sim_data_1.6$S)),
  K_A = length(unique(sim_data_1.6$analyte_abbr)),
  K_B = length(unique(sim_data_1.6$block)),
  K_T = length(unique(sim_data_1.6$treatment))
)


# Non-centered version
m1.6_nc <- ulam(
  alist(
    # Observation model
    #C_obs ~ dnorm(mu, sigma),
    C_obs ~ normal(mu, sigma_analyte[A]),
    
    # Mean structure with correlations between analytes, samplers, and blocks
    mu <- alpha[A] + beta[A, S] + gamma[A, B] + delta[A, TRT],
    
    # Analyte-specific (9 levels) intercepts (non-centered parameterization)
    transpars> vector[K_A]:alpha <<- mu_alpha + z_alpha * sigma_alpha,
    vector[K_A]:z_alpha ~ normal(0, 1),  # Non-centered scaling
    mu_alpha ~ normal(0, 1),  # Prior for mean intercepts
    sigma_alpha ~ exponential(1),  # Prior for intercept variation
    
    # Analyte-specific (9 levels) sampler (4 levels) effects (non-centered parameterization)
    transpars> matrix[K_A, K_S]:beta <<- mu_beta + z_beta * diag_pre_multiply(sigma_beta, L_beta),
    transpars> matrix[K_A, K_S]:mu_beta <- rep_matrix(0, K_A, K_S),  # Center at zero
    matrix[K_A, K_S]:z_beta ~ normal(0, 1),  # Standardized effects
    transpars> cholesky_factor_corr[K_S]:L_beta ~ lkj_corr_cholesky(2),  # LKJ prior
    vector[K_S]:sigma_beta ~ exponential(1),  # Prior for scaling sampler effects
    
    # Analyte-specific (9 levels) block (2 levels) effects (non-centered parameterization)
    transpars> matrix[K_A, K_B]:gamma <<- mu_gamma + z_gamma * diag_pre_multiply(sigma_gamma, L_gamma),
    transpars> matrix[K_A, K_B]:mu_gamma <- rep_matrix(0, K_A, K_B),  # Center at zero
    matrix[K_A, K_B]:z_gamma ~ normal(0, 1),  # Standardized effects
    transpars> cholesky_factor_corr[K_B]:L_gamma ~ lkj_corr_cholesky(2),  # LKJ prior
    vector[K_B]:sigma_gamma ~ exponential(1),  # Prior for scaling block effects
    
    # Analyte-specific (9 levels) treatment (3 levels) effects (non-centered parameterization)
    transpars> matrix[K_A, K_T]:delta <<- mu_delta + z_delta * diag_pre_multiply(sigma_delta, L_delta),
    transpars> matrix[K_A, K_T]:mu_delta <- rep_matrix(0, K_A, K_T),  # Center at zero
    matrix[K_A, K_T]:z_delta ~ normal(0, 1),  # Standardized effects
    transpars> cholesky_factor_corr[K_T]:L_delta ~ lkj_corr_cholesky(2),  # LKJ prior
    vector[K_T]:sigma_delta ~ exponential(1),  # Prior for scaling treatment effects
    
    # Prior for measurement error
    vector[K_A]:sigma_analyte ~ exponential(1)
    #sigma ~ exponential(1)
  ),
  data = data1.6,
  chains = 4,
  cores = 4,
  threads = 1,
  iter = 1000,              # Total iterations (increase this)
  warmup = 500             # Number of warmup iterations (optional, default is iter/2)
)

```

- Summary of results
```{r, eval=TRUE}
precis(m1.6_nc, depth = 2)
traceplot(m1.6_nc)
```

- Posterior predictions
```{r, eval=TRUE}
# Extract posterior samples
post1.6_nc <- extract.samples(m1.6_nc)

# Updated link function for posterior predictions (including treatment effects)
link_function_1.6_nc <- function(post1.6_nc, data) {
  n_samples <- nrow(post1.6_nc$alpha)       # Number of posterior samples
  n_analytes <- data$K_A                   # Number of analytes
  n_samplers <- data$K_S                   # Number of sampler types
  n_blocks <- data$K_B                     # Number of blocks
  n_treatments <- data$K_T                 # Number of treatments
  
  # Initialize predictions array
  predictions_nc <- array(NA, dim = c(n_samples, n_analytes, n_samplers, n_blocks, n_treatments))
  
  # Generate predictions
  for (a in 1:n_analytes) {
    for (s in 1:n_samplers) {
      for (b in 1:n_blocks) {
        for (t in 1:n_treatments) {
          # Compute predictions using the updated model structure with treatment effects
          predictions_nc[, a, s, b, t] <- post1.6_nc$mu_alpha +
                                          post1.6_nc$z_alpha[, a] * post1.6_nc$sigma_alpha +
                                          post1.6_nc$beta[, a, s] +
                                          post1.6_nc$gamma[, a, b] +
                                          post1.6_nc$delta[, a, t]
        }
      }
    }
  }
  
  return(predictions_nc)
}

# Generate predictions
posterior_predictions_1.6_nc <- link_function_1.6_nc(post1.6_nc, data1.6)

# Initialize array for back-transformed predictions
back_transformed_predictions_1.6_nc <- array(NA, dim = dim(posterior_predictions_1.6_nc))

# Back-transform predictions for each analyte
for (a in 1:data1.6$K_A) {
  # Extract mean and SD for the analyte from `sim_data_1.6`
  analyte_mean <- mean(sim_data_1.6$C_obs[sim_data_1.6$analyte_abbr == a])
  analyte_sd <- sd(sim_data_1.6$C_obs[sim_data_1.6$analyte_abbr == a])
  analyte_max <- max(sim_data_1.6$C_obs[sim_data_1.6$analyte_abbr == a])
  analyte_min <- min(sim_data_1.6$C_obs[sim_data_1.6$analyte_abbr == a])

  # Back-transform predictions for the analyte
  back_transformed_predictions_1.6_nc[, a, , , ] <- posterior_predictions_1.6_nc[, a, , , ] * analyte_sd + analyte_mean
  # back_transformed_predictions_1.6_nc[, a, , , ] <- posterior_predictions_1.6_nc[, a, , , ] * analyte_max # uncomment to use max regularization
}

# Summarize posterior predictions
# Initialize a summary dataframe
prediction_summary_1.6_nc <- data.frame()

# Summarize predictions for each analyte, sampler, block, and treatment
for (a in 1:data1.6$K_A) {
  for (s in 1:data1.6$K_S) {
    for (b in 1:data1.6$K_B) {
      for (t in 1:data1.6$K_T) {
        # Extract samples for the current analyte, sampler, block, and treatment
        samples <- back_transformed_predictions_1.6_nc[, a, s, b, t]

        # Add summary statistics to the dataframe
        summary_row <- data.frame(
          analyte_abbr = a,
          sampler_type = s,
          block = b,
          treatment = t,
          mean_prediction = mean(samples),
          prediction_5.5 = quantile(samples, 0.055),
          prediction_94.5 = quantile(samples, 0.945)
        )

        prediction_summary_1.6_nc <- rbind(prediction_summary_1.6_nc, summary_row)
      }
    }
  }
}

# Refine the summary table: Rows as analytes, columns as sampler types, with treatment and block information
refined_summary_table_1.6_nc <- prediction_summary_1.6_nc %>%
  select(analyte_abbr, sampler_type, block, treatment, mean_prediction) %>%
  pivot_wider(names_from = sampler_type, values_from = mean_prediction)

# View the refined summary table
refined_summary_table_1.6_nc
```

Results:

## Final model selection, graphing, and interpretation - TEST DATA
Select final model and 'observed data' for comparison
```{r}
# Specify the model to use
which_model <- "1.6_nc"  # Potential options: "1.0", "1.1", "1.2", "1.3", "1.4_c", "1.4_nc", "1.5_nc", "1.6_nc"

# Dynamically construct variable names and access them using get()
final_model <- get(paste0("m", which_model))  # e.g., "m1.4_nc"
final_post <- get(paste0("post", which_model))  # e.g., "post1.4_nc"
final_post_back_transformed <- get(paste0("back_transformed_predictions_", which_model))  # e.g., "back_transformed_predictions1.4_nc"
final_data <- get(paste0("sim_data_", gsub("_.*", "", which_model)))  # e.g., "sim_data_1.4"

# Export final data if needed
#write.csv(final_data, "../1_data/final_sim_wq_data.csv")

```

*Model Summary*
Plot Model Summary and Convergence Diagnostics
```{r}
precis(final_model, depth = 2)
plot(precis(final_model, depth = 2))
```

*Convergence Diagnostics*
```{r}
traceplot(final_model)
#trankplot(final_model)
```

*Plot Sampler Effects from Posterior*
```{r}
# Generic sampler effects plot, works for sim and real posteriors
plot_sampler_effects <- function(posterior_samples, sampler_labels) {

  n_samplers <- dim(posterior_samples$beta)[3]  # sampler types
  n_analytes <- dim(posterior_samples$beta)[2]  # analytes
  n_samples  <- dim(posterior_samples$beta)[1]  # posterior draws

  # Detect if treatment effects are included
  include_treatment <- "delta" %in% names(posterior_samples)

  # Average sampler effects across analytes, optionally including treatment effects
  sampler_effects <- matrix(NA, nrow = n_samples, ncol = n_samplers)
  for (s in 1:n_samplers) {
    if (include_treatment) {
      # Average treatment effect over treatments for each sample/analyte
      delta_mean <- apply(posterior_samples$delta, c(1, 2), mean)
      sampler_effects[, s] <- rowMeans(
        posterior_samples$beta[, , s] + delta_mean,
        na.rm = TRUE
      )
    } else {
      sampler_effects[, s] <- rowMeans(posterior_samples$beta[, , s],
                                       na.rm = TRUE)
    }
  }

  # Dynamic x axis limits
  percent_buffer <- 0.10
  x_min <- min(sampler_effects, na.rm = TRUE)
  x_max <- max(sampler_effects, na.rm = TRUE)
  xlim <- c(
    x_min - percent_buffer * abs(x_min),
    x_max + percent_buffer * abs(x_max)
  )

  # Dynamic y axis limits
  y_max <- max(
    apply(sampler_effects, 2, function(col) max(density(col)$y, na.rm = TRUE)),
    na.rm = TRUE
  )
  ylim <- c(0, y_max * 1.1)

  # Accessibility friendly palette and styling
  colors <- c(
    "#005AB5",  # blue
    "#DC3220",  # red
    "#009E73",  # green
    "#E69F00",  # orange
    "#CC79A7",  # magenta
    "#56B4E9"   # light blue
  )

  ltype  <- c(1, 3, 4, 6)
  lthick <- c(3, 3, 2.5, 4)

  # Base plot
  plot(NULL, xlim = xlim, ylim = ylim,
       xlab = "Sampler effect (averaged over analytes, blocks, treatments)",
       ylab = "Density",
       main = "Posterior sampler effects")

  # Posterior densities
  for (s in 1:n_samplers) {
    col_s <- colors[(s - 1) %% length(colors) + 1]
    lt_s  <- ltype[(s - 1) %% length(ltype) + 1]
    lw_s  <- lthick[(s - 1) %% length(lthick) + 1]

    dens(sampler_effects[, s],
         col = col_s,
         lwd = lw_s,
         lty = lt_s,
         add = TRUE)
  }

  # Prior density line
  curve(dnorm(x, mean = 0, sd = 1),
        from = xlim[1], to = xlim[2],
        lwd = 2, lty = 2, col = "black", add = TRUE)

  # Legend
  cols_used <- sapply(
    1:n_samplers,
    function(s) colors[(s - 1) %% length(colors) + 1]
  )
  lwd_used <- sapply(
    1:n_samplers,
    function(s) lthick[(s - 1) %% length(lthick) + 1]
  )
  lty_used <- sapply(
    1:n_samplers,
    function(s) ltype[(s - 1) %% length(ltype) + 1]
  )

  legend("topright",
         legend = c(sampler_labels, "Prior"),
         col    = c(cols_used, "black"),
         lwd    = c(lwd_used, 2),
         lty    = c(lty_used, 2))
}


# Example usage
sampler_types <- levels(as.factor(final_data$S))

png('../figs/sampler_effects_sim.png', width = 1600, height = 1200, res = 300)
plot_sampler_effects(posterior_samples = final_post,
                     sampler_labels   = sampler_types)
dev.off()

plot_sampler_effects(posterior_samples = final_post,
                     sampler_labels   = sampler_types)
```

*Plot Sampler Effect Contrasts*
```{r}
# Generic sampler effect contrasts plot
# Works for simulated and real posteriors
plot_sampler_contrasts <- function(posterior_samples, sampler_labels) {

  # Extract sampler effects from posterior samples
  n_samplers <- dim(posterior_samples$beta)[3]
  n_samples  <- dim(posterior_samples$beta)[1]

  sampler_effects <- matrix(NA, nrow = n_samples, ncol = n_samplers)
  for (s in 1:n_samplers) {
    # Mean sampler effect across analytes
    # (If delta is present and independent of sampler, it cancels in contrasts anyway.)
    sampler_effects[, s] <- rowMeans(posterior_samples$beta[, , s], na.rm = TRUE)
  }

  # Compute all pairwise contrasts
  contrast_list <- list()
  for (i in 1:(n_samplers - 1)) {
    for (j in (i + 1):n_samplers) {
      cname <- paste(sampler_labels[i], "-", sampler_labels[j])
      contrast_list[[cname]] <- sampler_effects[, i] - sampler_effects[, j]
    }
  }

  # Dynamic x-axis limits with minimum buffer
  all_contrasts  <- unlist(contrast_list)
  percent_buffer <- 0.10
  min_buffer     <- 0.10

  x_min   <- min(all_contrasts, na.rm = TRUE)
  x_max   <- max(all_contrasts, na.rm = TRUE)
  x_range <- x_max - x_min

  xlim <- c(
    x_min - max(percent_buffer * x_range, min_buffer),
    x_max + max(percent_buffer * x_range, min_buffer)
  )

  # Dynamic y-axis limits
  y_max <- 0
  for (k in seq_along(contrast_list)) {
    d <- density(contrast_list[[k]])
    y_max <- max(y_max, max(d$y, na.rm = TRUE))
  }
  ylim <- c(0, y_max * 1.1)

  # Accessibility-friendly palette and styling (shared across plots)
  colors <- c(
    "#005AB5",  # blue
    "#DC3220",  # red
    "#009E73",  # green
    "#E69F00",  # orange
    "#CC79A7",  # magenta
    "#56B4E9"   # light blue
  )

  ltype  <- c(1, 3, 4, 6)
  lthick <- c(3, 3, 2.5, 4)

  # Base plot
  plot(NULL, xlim = xlim, ylim = ylim,
       xlab = "Contrast effect",
       ylab = "Density",
       main = "Posterior sampler contrasts")

  # Plot each contrast with consistent styles
  for (k in seq_along(contrast_list)) {
    contrast <- contrast_list[[k]]

    col_k <- colors[(k - 1) %% length(colors) + 1]
    lt_k  <- ltype[(k - 1) %% length(ltype) + 1]
    lw_k  <- lthick[(k - 1) %% length(lthick) + 1]

    dens(contrast, col = col_k, lwd = lw_k, lty = lt_k, add = TRUE)
  }

  # Legend with matching styles
  legend(
    "topright",
    legend = names(contrast_list),
    col    = sapply(seq_along(contrast_list),
                    function(k) colors[(k - 1) %% length(colors) + 1]),
    lwd    = sapply(seq_along(contrast_list),
                    function(k) lthick[(k - 1) %% length(lthick) + 1]),
    lty    = sapply(seq_along(contrast_list),
                    function(k) ltype[(k - 1) %% length(ltype) + 1])
  )
}


# Example usage
sampler_types <- levels(as.factor(final_data$S))

png('../figs/sampler_contrasts_sim.png', width = 1600, height = 1200, res = 300)
plot_sampler_contrasts(
  posterior_samples = final_post,
  sampler_labels    = sampler_types
)
dev.off()
plot_sampler_contrasts(
  posterior_samples = final_post,
  sampler_labels    = sampler_types
)

```

*Plot Treatment Effects*
```{r}
# Generic treatment effects plot
# Works for simulated and real posteriors
plot_treatment_effects <- function(posterior_samples, treatment_labels) {
  if (!"delta" %in% names(posterior_samples)) {
    stop("Treatment effects 'delta' not found in posterior samples.")
  }

  n_treatments <- dim(posterior_samples$delta)[3]  # treatments
  n_analytes   <- dim(posterior_samples$delta)[2]  # analytes
  n_samples    <- dim(posterior_samples$delta)[1]  # posterior draws

  # Average treatment effects across analytes for each treatment
  treatment_effects <- matrix(NA, nrow = n_samples, ncol = n_treatments)
  for (t in 1:n_treatments) {
    treatment_effects[, t] <- rowMeans(posterior_samples$delta[, , t],
                                       na.rm = TRUE)
  }

  # Dynamic x-axis limits (consistent with sampler plots)
  percent_buffer <- 0.10
  x_min <- min(treatment_effects, na.rm = TRUE)
  x_max <- max(treatment_effects, na.rm = TRUE)
  xlim <- c(
    x_min - percent_buffer * abs(x_min),
    x_max + percent_buffer * abs(x_max)
  )

  # Dynamic y-axis limits
  y_max <- 0
  for (t in 1:n_treatments) {
    dens_data <- density(treatment_effects[, t])
    y_max <- max(y_max, max(dens_data$y, na.rm = TRUE))
  }
  ylim <- c(0, y_max * 1.1)

  # Accessibility-friendly palette and styling (shared across plots)
  colors <- c(
    "#005AB5",  # blue
    "#DC3220",  # red
    "#009E73",  # green
    "#E69F00",  # orange
    "#CC79A7",  # magenta
    "#56B4E9"   # light blue
  )

  ltype  <- c(1, 3, 4, 6)    # line types
  lthick <- c(3, 3, 2.5, 4)  # line thicknesses

  plot(NULL, xlim = xlim, ylim = ylim,
       xlab = "Treatment effect (averaged over analytes)",
       ylab = "Density",
       main = "Posterior treatment effects")

  # Posterior densities for each treatment
  for (t in 1:n_treatments) {
    col_t <- colors[(t - 1) %% length(colors) + 1]
    lt_t  <- ltype[(t - 1) %% length(ltype) + 1]
    lw_t  <- lthick[(t - 1) %% length(lthick) + 1]

    dens(treatment_effects[, t],
         col = col_t,
         lwd = lw_t,
         lty = lt_t,
         add = TRUE)
  }

  # Standard normal prior density
  curve(dnorm(x, mean = 0, sd = 1),
        from = xlim[1], to = xlim[2],
        lwd = 2, lty = 2, col = "black", add = TRUE)

  # Legend styling consistent with lines above
  colors_used <- sapply(
    1:n_treatments,
    function(t) colors[(t - 1) %% length(colors) + 1]
  )
  lwd_used <- sapply(
    1:n_treatments,
    function(t) lthick[(t - 1) %% length(lthick) + 1]
  )
  lty_used <- sapply(
    1:n_treatments,
    function(t) ltype[(t - 1) %% length(ltype) + 1]
  )

  legend("topright",
         legend = c(treatment_labels, "Prior"),
         col    = c(colors_used, "black"),
         lwd    = c(lwd_used, 2),
         lty    = c(lty_used, 2))
}

# Example usage
# if trt_mapping is like c("Control" = 1, "Manure" = 2, ...)
treatment_labels <- names(trt_mapping)

png('../figs/treatment_effects_sim.png', width = 2000, height = 1200, res = 300)
plot_treatment_effects(final_post, treatment_labels)
dev.off()

plot_treatment_effects(final_post, treatment_labels)


```


*Plot Concentration Predictions*
```{r}
# Flexible function for predictions with or without treatment effects
plot_predictions_with_observed <- function(back_transformed_predictions,
                                           data,
                                           sampler_labels,          # e.g. c("LCS","ISCO","GB","GBH")
                                           analyte_index,
                                           block_index    = NULL,   # numeric or "Block1"/"Block2"
                                           treatment_index = NULL,  # numeric or "CT"/"MT"/"ST"
                                           sampler_mapping = NULL   # optional: named numeric mapping
) {
  # Analyte labels (with units, works fine for both sim and real)
  analyte_dict <- c(
    "NO3 (mg/L)", "NO2 (mg/L)", "TKN (mg/L)", "pH",
    "TP (mg/L)", "OP (mg/L)", "EC (dS/m)", "TSS (mg/L)", "TDS (mg/L)"
  )
  analyte_name <- ifelse(
    analyte_index > 0 & analyte_index <= length(analyte_dict),
    analyte_dict[analyte_index],
    "Unknown"
  )

  # Block and Treatment levels (if present)
  block_levels     <- if ("block"    %in% names(data)) levels(data$block) else NULL
  treatment_levels <- if ("treatment" %in% names(data)) levels(as.factor(data$treatment)) else NULL

  # Normalise block index for naming and numeric index for array use
  if (!is.null(block_levels) && !is.null(block_index) && is.character(block_index)) {
    block_index_num <- match(block_index, block_levels)
    block_name      <- block_index
  } else if (!is.null(block_levels) && !is.null(block_index)) {
    block_index_num <- block_index
    block_name      <- block_levels[block_index]
  } else {
    block_index_num <- NULL
    block_name      <- "All Blocks"
  }

  # Normalise treatment index for naming and numeric index for array use
  if (!is.null(treatment_levels) && !is.null(treatment_index) && is.character(treatment_index)) {
    treatment_index_num <- match(treatment_index, treatment_levels)
    treatment_name      <- treatment_index
  } else if (!is.null(treatment_levels) && !is.null(treatment_index)) {
    treatment_index_num <- treatment_index
    treatment_name      <- treatment_levels[treatment_index]
  } else {
    treatment_index_num <- NULL
    treatment_name      <- "All Treatments"
  }

  # Determine structure of posterior predictions
  dims          <- dim(back_transformed_predictions)
  n_dims        <- length(dims)
  n_samplers    <- dims[3]
  predictions   <- list()

  # Extract predictions for each sampler given model structure
  for (s in 1:n_samplers) {
    if (n_dims == 5) {
      # [draw, analyte, sampler, block, treatment]
      if (!is.null(block_index_num) && !is.null(treatment_index_num)) {
        vals <- back_transformed_predictions[, analyte_index, s,
                                             block_index_num, treatment_index_num]
      } else if (!is.null(block_index_num) && is.null(treatment_index_num)) {
        # average over treatments, given block
        vals <- apply(
          back_transformed_predictions[, analyte_index, s, block_index_num, , drop = FALSE],
          1, mean, na.rm = TRUE
        )
      } else {
        # no block or treatment specified, average over both
        vals <- apply(
          back_transformed_predictions[, analyte_index, s, , , drop = FALSE],
          1, mean, na.rm = TRUE
        )
      }
    } else if (n_dims == 4) {
      # [draw, analyte, sampler, block]
      if (!is.null(block_index_num)) {
        vals <- back_transformed_predictions[, analyte_index, s, block_index_num]
      } else {
        vals <- back_transformed_predictions[, analyte_index, s, 1]
      }
    } else if (n_dims == 3) {
      # [draw, analyte, sampler]
      vals <- back_transformed_predictions[, analyte_index, s]
    } else {
      stop("Unexpected dimension of back_transformed_predictions.")
    }

    predictions[[sampler_labels[s]]] <- vals
  }

  # Dynamic axis limits
  all_predictions <- unlist(predictions)
  percent_buffer  <- 0.10
  x_min           <- min(all_predictions, na.rm = TRUE)
  x_max           <- max(all_predictions, na.rm = TRUE)
  x_range         <- x_max - x_min

  xlim <- c(
    x_min - max(percent_buffer * x_range, 0.10),
    x_max + max(percent_buffer * x_range, 0.10)
  )

  # y-axis limit
  y_max <- max(
    sapply(predictions, function(p) max(density(p)$y, na.rm = TRUE)),
    na.rm = TRUE
  )
  ylim <- c(0, y_max * 1.1)

  # Accessibility-friendly palette and styling (same as other plots)
  colors <- c(
    "#005AB5",  # blue
    "#DC3220",  # red
    "#009E73",  # green
    "#E69F00",  # orange
    "#CC79A7",  # magenta
    "#56B4E9"   # light blue
  )

  ltype  <- c(1, 3, 4, 6)
  lthick <- c(3, 3, 2.5, 4)

  # Base plot
  plot(NULL, xlim = xlim, ylim = ylim,
       xlab = "Predicted concentration",
       ylab = "Density",
       main = sprintf("Posterior predictions with observed means\nAnalyte: %s | Block: %s | Treatment: %s",
                      analyte_name, block_name, treatment_name))

  # Posterior prediction densities
  for (s in 1:n_samplers) {
    col_s <- colors[(s - 1) %% length(colors) + 1]
    lt_s  <- ltype[(s - 1) %% length(ltype) + 1]
    lw_s  <- lthick[(s - 1) %% length(lthick) + 1]

    dens(predictions[[sampler_labels[s]]],
         col = col_s,
         lwd = lw_s,
         lty = lt_s,
         add = TRUE)
  }

  # Figure out which column contains the analyte index in the data
  analyte_col <- if ("A" %in% names(data)) {
    "A"                  # numeric index: matches analyte_mapping
  } else if ("analyte_abbr" %in% names(data)) {
    "analyte_abbr"       # character abbreviation, used in sim data
  } else {
    stop("Data must contain either 'A' or 'analyte_abbr' for analyte indices.")
  }


  # Observed means as vertical dashed lines
  for (s in 1:n_samplers) {
    col_s <- colors[(s - 1) %% length(colors) + 1]

    # Start from all rows
    observed_data <- data

    # Filter by analyte index
    observed_data <- observed_data[observed_data[[analyte_col]] == analyte_index, , drop = FALSE]

      # Filter by sampler: either via mapping (numeric codes) or direct labels
      if (!is.null(sampler_mapping)) {
        # numeric codes, e.g., 1,2,3,4 in d$S
        sampler_code <- sampler_mapping[sampler_labels[s]]
        observed_data <- observed_data[observed_data$S == sampler_code, , drop = FALSE]
      } else {
        # labels, e.g., "LCS","ISCO" in final_data$S
        observed_data <- observed_data[as.character(observed_data$S) == sampler_labels[s], , drop = FALSE]
      }

    # Filter by block, if specified
    if (!is.null(block_index_num) && !is.null(block_levels)) {
      observed_data <- observed_data[observed_data$block == block_levels[block_index_num], , drop = FALSE]
    }

    # Filter by treatment, if specified
    if (!is.null(treatment_index_num) && !is.null(treatment_levels)) {
      observed_data <- observed_data[observed_data$treatment == treatment_levels[treatment_index_num], , drop = FALSE]
    }

    observed_values <- observed_data$C_obs
    if (length(observed_values) > 0) {
      observed_mean <- mean(observed_values, na.rm = TRUE)
      abline(v = observed_mean,
             col = col_s,
             lwd = 3,
             lty = 1)  # dashed for observed mean
    }
  }

  # Legend: sampler curves (posterior predictions)
  cols_used <- sapply(1:n_samplers,
                      function(s) colors[(s - 1) %% length(colors) + 1])
  lwd_used  <- sapply(1:n_samplers,
                      function(s) lthick[(s - 1) %% length(lthick) + 1])
  lty_used  <- sapply(1:n_samplers,
                      function(s) ltype[(s - 1) %% length(ltype) + 1])

  legend("topright",
         legend = sampler_labels,
         col    = cols_used,
         lwd    = lwd_used,
         lty    = lty_used)
}

```

```{r}
# ------------------------
# Allowed Levels for Each Parameter:
# ------------------------
# analyte_index: 1 to 9 ("NO3", "NO2", "TKN", "pH", "TP", "OP", "EC", "TSS", "TDS")
# block_index: "Block1", "Block2" (or their numeric indices 1, 2)
# treatment_index (for models with treatments): "CT", "MT", "ST" (or their numeric indices 1, 2, 3)
# sampler_types: "LCS", "ISCO", "GB", "GBH"

# Notes:
# - For models **without treatment effects** (e.g., m1.4_nc), simply omit the treatment_index argument.
# - For models **without block effects**, you can omit block_index as well.
# - The function automatically adjusts based on the model structure (number of dimensions in the posterior predictions).

# ------------------------
# Example Run: Model with Treatments (e.g., m1.6_nc)
# ------------------------
sampler_labels <- names(sampler_mapping)

png("../figs/predictions_with_sim.png",
    width = 1600, height = 1200, res = 300)
plot_predictions_with_observed(
  back_transformed_predictions = final_post_back_transformed,
  data           = final_data,
  sampler_labels = sampler_labels,
  analyte_index  = 1,          # NO3
  block_index    = "Block1",
  #treatment_index = "MT"       # or NULL to average over treatments
)
dev.off()
plot_predictions_with_observed(
  back_transformed_predictions = final_post_back_transformed,
  data           = final_data,
  sampler_labels = sampler_labels,
  analyte_index  = 1,          # NO3
  block_index    = "Block1",
  #treatment_index = "MT"       # or NULL to average over treatments
)
```


## Final model selection, graphing, and interpretation - REAL DATA

Run selected model using real data - which imputes missing data from Storm 1,2
```{r}
# Data is 'dat' as created from the real data in the Tools section

# Flexible model update function
update_model_with_real_data <- function(model_name, real_data) {
  model_object <- get(model_name)  # Retrieve the model object using its name
  updated_model <- update(model_object, data = real_data, iter = 4000, warmup = 1000, chains = 4, cores = 4, threads = 1)
  return(updated_model)
}

# Define the model to use (change this variable to switch models)
selected_model_name <- "m1.6_nc"  # Example: "m1.4_nc", "m1.6_nc", etc.

# Update the selected model with real data
updated_model_real <- update_model_with_real_data(selected_model_name, dat)
```

Generate posterior predictions and back-transformed predictions
```{r}
# Flexible link function to handle models with and without treatment effects
link_function_flexible <- function(post, data) {
  n_samples <- nrow(post$beta[, , 1])  # Number of posterior samples
  n_analytes <- data$K_A               # Number of analytes
  n_samplers <- data$K_S               # Number of sampler types
  n_blocks <- data$K_B                 # Number of blocks

  # Detect if treatment effects are included
  include_treatment <- "delta" %in% names(post)
  n_treatments <- if (include_treatment) data$K_T else 1

  # Initialize predictions array
  predictions <- if (include_treatment) {
    array(NA, dim = c(n_samples, n_analytes, n_samplers, n_blocks, n_treatments))
  } else {
    array(NA, dim = c(n_samples, n_analytes, n_samplers, n_blocks))
  }

  # Generate predictions based on model structure
  for (a in 1:n_analytes) {
    for (s in 1:n_samplers) {
      for (b in 1:n_blocks) {
        if (include_treatment) {
          for (t in 1:n_treatments) {
            predictions[, a, s, b, t] <- post$mu_alpha + 
                                         post$z_alpha[, a] * post$sigma_alpha + 
                                         post$beta[, a, s] + 
                                         post$gamma[, a, b] + 
                                         post$delta[, a, t]
          }
        } else {
          predictions[, a, s, b] <- post$mu_alpha + 
                                    post$z_alpha[, a] * post$sigma_alpha + 
                                    post$beta[, a, s] + 
                                    post$gamma[, a, b]
        }
      }
    }
  }

  return(predictions)
}

# Extract posterior samples and generate predictions
posterior_samples_real <- extract.samples(updated_model_real)
posterior_predictions_real <- link_function_flexible(posterior_samples_real, dat)

# Back-transform predictions
back_transformed_predictions_real <- posterior_predictions_real
for (a in 1:dat$K_A) {
  analyte_mean <- mean(d$result[d$A == a], na.rm = TRUE)
  analyte_sd <- sd(d$result[d$A == a], na.rm = TRUE)
  analyte_max <- max(d$result[d$A == a], na.rm = TRUE)
  analyte_min <- min(d$result[d$A == a], na.rm = TRUE)

  if (length(dim(back_transformed_predictions_real)) == 4) {
    back_transformed_predictions_real[, a, , ] <- posterior_predictions_real[, a, , ] * analyte_sd + analyte_mean
    # back_transformed_predictions_real[, a, , ] <- posterior_predictions_real[, a, , ] * analyte_max # uncomment when using max transform
  } else {
    back_transformed_predictions_real[, a, , , ] <- posterior_predictions_real[, a, , , ] * analyte_sd + analyte_mean
    # back_transformed_predictions_real[, a, , , ] <- posterior_predictions_real[, a, , , ] * analyte_max # uncomment when using max transform
  }
}

# Initialize summary dataframe
prediction_summary <- data.frame()

# Summarize predictions (adjusting for treatment presence)
for (a in 1:dat$K_A) {
  for (s in 1:dat$K_S) {
    for (b in 1:dat$K_B) {
      if (length(dim(back_transformed_predictions_real)) == 4) {
        # Model without treatments
        samples <- back_transformed_predictions_real[, a, s, b]
        summary_row <- data.frame(
          analyte_abbr = a,
          sampler_type = as.character(levels(d$method.name)[s]),
          block = as.character(levels(d$block)[b]),
          mean_prediction = mean(samples, na.rm = TRUE),
          prediction_5.5 = quantile(samples, 0.055, na.rm = TRUE),
          prediction_94.5 = quantile(samples, 0.945, na.rm = TRUE)
        )
      } else {
        # Model with treatments
        for (t in 1:dat$K_T) {
          samples <- back_transformed_predictions_real[, a, s, b, t]
          summary_row <- data.frame(
            analyte_abbr = a,
            sampler_type = as.character(levels(d$method.name)[s]),
            block = as.character(levels(d$block)[b]),
            treatment = as.character(levels(as.factor(d$treatment))[t]),
            mean_prediction = mean(samples, na.rm = TRUE),
            prediction_5.5 = quantile(samples, 0.055, na.rm = TRUE),
            prediction_94.5 = quantile(samples, 0.945, na.rm = TRUE)
          )
        }
      }
      prediction_summary <- rbind(prediction_summary, summary_row)
    }
  }
}

# Refine the summary table
if ("treatment" %in% names(prediction_summary)) {
  refined_summary_table <- prediction_summary %>%
    select(analyte_abbr, sampler_type, mean_prediction, block, treatment) %>%
    pivot_wider(names_from = sampler_type, values_from = mean_prediction)
} else {
  refined_summary_table <- prediction_summary %>%
    select(analyte_abbr, sampler_type, mean_prediction, block) %>%
    pivot_wider(names_from = sampler_type, values_from = mean_prediction)
}

# View the refined summary table
refined_summary_table
```

Plot Model Summary and Convergence Diagnostics
*Model Summary*
```{r}
# Check summary of recalibrated model
precis(updated_model_real, depth = 2)
plot(precis(updated_model_real, depth = 1))
```

*Convergence Diagnostics*
```{r}
#traceplot(updated_model_real)
#trankplot(updated_model_real)
```

*Plot Sampler Effects from Posterior*
```{r}
# Plot sampler effects with dynamic axis limits
sampler_mapping <- c("LCS" = 1, "ISCO" = 2, "GB" = 3, "GBH" = 4)
sampler_labels  <- names(sampler_mapping)

jpeg('../figs/sampler_effects.jpg', width = 2000, height = 1200, res = 300)
plot_sampler_effects(posterior_samples = posterior_samples_real,
                     sampler_labels    = sampler_labels)
dev.off()
plot_sampler_effects(posterior_samples = posterior_samples_real,
                     sampler_labels    = sampler_labels)
```

*Plot Sampler Effect Contrasts*
```{r}
# Plot Sampler Effect Contrasts (Averaged over Analytes, Blocks, and Treatments)
sampler_mapping <- c("LCS" = 1, "ISCO" = 2, "GB" = 3, "GBH" = 4)
sampler_labels  <- names(sampler_mapping)

jpeg('../figs/sampler_contrasts.png', width = 1600, height = 1200, res = 300)
plot_sampler_contrasts(
  posterior_samples = posterior_samples_real,
  sampler_labels    = sampler_labels
)
dev.off()
plot_sampler_contrasts(
  posterior_samples = posterior_samples_real,
  sampler_labels    = sampler_labels
)
```

*Plot Treatment Effects*
```{r}

# Real data posterior
treatment_labels <- names(trt_mapping)  # or whatever mapping you use

png('../figs/treatment_effects_real.png', width = 2000, height = 1200, res = 300)
plot_treatment_effects(posterior_samples_real, treatment_labels)
dev.off()
plot_treatment_effects(posterior_samples_real, treatment_labels)

```

*Plot Concentration Predictions*

```{r}
# Loop through each analyte and generate a plot
for (analyte_name in names(analyte_mapping)) {
  analyte_index <- analyte_mapping[[analyte_name]]

  # output_file <- paste0("../figs/predictions_with_observed_noDupe_", analyte_name, ".png")
  output_file <- paste0("../figs/predictions_with_observed_", analyte_name, ".png")

  png(output_file, width = 1900, height = 1200, res = 300)
  plot_predictions_with_observed(
    back_transformed_predictions = back_transformed_predictions_real,
    data            = d,
    sampler_labels  = sampler_labels,
    sampler_mapping = sampler_mapping,    # <- add this
    analyte_index   = analyte_index,
    block_index     = "Block1" # comment out to avg over blocks
    #treatment_index = "CT"
  )
  dev.off()

  cat("Saved:", output_file, "\n")
}


```

**Individual tool**
```{r}
# Example: Plot specific analyte, block, and treatment
plot_predictions_with_observed(back_transformed_predictions_real,
                               data = d,
                               sampler_labels = sampler_labels,
                               analyte_index = 1,  # NO3
                               block_index    = NULL,
                               treatment_index = NULL,
                               sampler_mapping = NULL)

```

**Facet Plot**
```{r}
plot_posterior_facets <- function(
  posterior_array,
  sampler_mapping,
  analyte_mapping,   # kept for interface consistency, not used directly
  observed_means,
  colors = c(
    "#005AB5",  # blue
    "#DC3220",  # red
    "#009E73",  # green
    "#E69F00",  # orange
    "#CC79A7",  # magenta
    "#56B4E9"   # light blue
  ),
  ltype  = c(1, 3, 4, 6),
  lthick = c(3, 3, 2.5, 4),
  ylab   = "Density",
  n_cols = 3
) {
  n_analyte <- dim(posterior_array)[2]
  n_sampler <- dim(posterior_array)[3]
  sampler_labels <- names(sampler_mapping)
  
  analyte_names <- c("NO3", "NO2", "TKN", "pH", "TP", "OP", "EC", "TSS", "TDS")
  analyte_axis_labels <- c(
    "mg/L", "mg/L", "mg/L", "", "mg/L", "mg/L", "dS/m", "mg/L", "mg/L"
  )
  
  n_rows <- ceiling(n_analyte / n_cols)
  
  old_par <- par(no.readonly = TRUE)
  on.exit(par(old_par))
  
  par(mfrow = c(n_rows, n_cols), mar = c(4, 3, 2, 1), oma = c(6, 3, 1, 2))
  
  for (a in 1:n_analyte) {
    # Build densities for each sampler, averaging over block/treatment if present
    dens_list <- lapply(1:n_sampler, function(s) {
      vals <- if (length(dim(posterior_array)) > 3) {
        # e.g., [draw, analyte, sampler, block, treatment] -> average over block,treatment
        apply(posterior_array[, a, s, , , drop = FALSE],
              1, mean, na.rm = TRUE)
      } else {
        posterior_array[, a, s]
      }
      density(vals, na.rm = TRUE)
    })
    
    xlim <- range(unlist(lapply(dens_list, function(d) d$x)), na.rm = TRUE)
    ylim <- range(unlist(lapply(dens_list, function(d) d$y)), na.rm = TRUE)
    
    # First sampler curve
    col1 <- colors[1]
    lt1  <- ltype[1]
    lw1  <- lthick[1]
    plot(
      dens_list[[1]]$x, dens_list[[1]]$y,
      type = "l",
      lwd  = lw1,
      lty  = lt1,
      col  = col1,
      xlim = xlim,
      ylim = ylim,
      xlab = "",
      ylab = ylab,
      main = analyte_names[a],
      cex.main = 1.1
    )
    
    # Remaining sampler curves
    if (n_sampler > 1) {
      for (s in 2:n_sampler) {
        col_s <- colors[(s - 1) %% length(colors) + 1]
        lt_s  <- ltype[(s - 1) %% length(ltype) + 1]
        lw_s  <- lthick[(s - 1) %% length(lthick) + 1]
        lines(dens_list[[s]]$x, dens_list[[s]]$y,
              lwd = lw_s,
              lty = lt_s,
              col = col_s)
      }
    }
    
    # Observed means as vertical lines
    for (s in 1:n_sampler) {
      col_s <- colors[(s - 1) %% length(colors) + 1]
      obs_val <- observed_means %>%
        dplyr::filter(analyte_type == analyte_names[a],
                      sampler_type == sampler_labels[s]) %>%
        dplyr::pull(observed_mean)
      if (length(obs_val) == 1 && !is.na(obs_val)) {
        abline(v = obs_val, col = col_s, lwd = 2, lty = 1)
      }
    }
    
    mtext(analyte_axis_labels[a], side = 1, line = 2, cex = 0.9)
  }
  
  # Draw global legend in outer margin (below)
  par(xpd = NA)
  cols_used <- sapply(1:n_sampler,
                      function(s) colors[(s - 1) %% length(colors) + 1])
  lwd_used  <- sapply(1:n_sampler,
                      function(s) lthick[(s - 1) %% length(lthick) + 1])
  lty_used  <- sapply(1:n_sampler,
                      function(s) ltype[(s - 1) %% length(ltype) + 1])
  
  legend(
    x = grconvertX(-0.6, from = "npc", to = "user"),
    y = grconvertY(-0.5, from = "npc", to = "user"),
    legend = sampler_labels,
    col    = cols_used,
    lwd    = lwd_used,
    lty    = lty_used,
    horiz  = TRUE,
    bty    = "n",
    cex    = 1.3,
    xjust  = 0.5
  )
  par(xpd = FALSE)
}


```

```{r}
plot_posterior_facets(
  posterior_array  = back_transformed_predictions_real,
  sampler_mapping  = sampler_mapping,
  analyte_mapping  = analyte_mapping,
  observed_means   = observed_means,
  n_cols           = 3
)


```

```{r}
jpeg(
  filename = "../figs/posterior_predictive_facets.jpg",
  width = 2700, height = 1950, res = 300, units = "px"
)
plot_posterior_facets(
  posterior_array  = back_transformed_predictions_real,
  sampler_mapping  = sampler_mapping,
  analyte_mapping  = analyte_mapping,
  observed_means   = observed_means,
  n_cols           = 3   # adjust as needed
  # colors, ltype, lthick can be overridden here if you ever want
)
dev.off()


```

*Export Predictions Summary*
```{r}
#' Summarize posterior predictions by sampler and analyte
#' @param back_transformed_predictions The posterior predictive array: draws  analyte  sampler  (block)  (treatment)
#' @param sampler_mapping Named vector or list mapping sampler indices to sampler names
#' @param analyte_mapping Named vector or list mapping analyte indices to analyte names
#' @param data Original data frame containing observed concentrations and sampler/analyte mappings
#' @return Data frame with sampler, analyte, mean, lower_95ci, upper_95ci
summarize_predictions_with_observed <- function(
  back_transformed_predictions, sampler_mapping, analyte_mapping, data
) {
  n_dims <- length(dim(back_transformed_predictions))
  draws <- dim(back_transformed_predictions)[1]
  n_analyte <- dim(back_transformed_predictions)[2]
  n_sampler <- dim(back_transformed_predictions)[3]

  result_list <- list()
  
  for (a in 1:n_analyte) {
    for (s in 1:n_sampler) {
      # Extract posterior draws, averaged over block/treatment if needed
      if (n_dims == 5) {
        vals <- apply(back_transformed_predictions[, a, s, , ], 1, mean, na.rm = TRUE)
      } else if (n_dims == 4) {
        vals <- apply(back_transformed_predictions[, a, s, , drop = FALSE], 1, mean, na.rm = TRUE)
      } else {
        vals <- back_transformed_predictions[, a, s]
      }
      mean_pred <- mean(vals, na.rm = TRUE)
      lower_95 <- quantile(vals, 0.025, na.rm = TRUE)
      upper_95 <- quantile(vals, 0.975, na.rm = TRUE)
      
      # Compute observed mean for this sampler and analyte, averaged over all blocks/treatments
      sampler_value <- sampler_mapping[[s]]
      analyte_value <- analyte_mapping[[a]]
      
      # Filter observed data
      obs_rows <- data[data$A == a & data$S == sampler_value, ]
      observed_mean <- mean(obs_rows$C_obs, na.rm = TRUE)
      
      result_list[[length(result_list) + 1]] <- data.frame(
        analyte_type = names(analyte_mapping)[a],
        sampler_type = names(sampler_mapping)[s],
        observed_mean = observed_mean,
        mean = mean_pred,
        lower_95ci = lower_95,
        upper_95ci = upper_95
      )
    }
  }
  
  results_df <- do.call(rbind, result_list)
  return(results_df)
}
```

```{r}
results_df <- summarize_predictions_with_observed(
  back_transformed_predictions = back_transformed_predictions_real,
  sampler_mapping = sampler_mapping,
  analyte_mapping = analyte_mapping,
  data = d
)
  
# view results
print(results_df)
# Export to CSV
write.csv(results_df, "../1_data/posterior_predictions_summary.csv", row.names = FALSE)

```

*Plot Concentration Predictions as Violin Plots*
```{r}
custom_colors <- c(
  "LCS" = "#8080FF",  # blue-purple
  "ISCO" = "#F98400", # orange
  "GB" = "#00A08A",   # teal
  "GBH" = "#E2AD00")  # gold

violin_edges <- c(
  "LCS" = "#4B4BB3",   # darker blue-purple
  "ISCO" = "#B35C00",  # darker orange
  "GB" = "#006655",    # darker teal
  "GBH" = "#B38F00"    # darker gold
)

plotly_violin_predictions <- function(back_transformed_predictions, data, sampler_mapping, analyte_index, block_index = NULL, treatment_index = NULL) {

  analyte_dict <- c("NO3 (mg/L)", "NO2 (mg/L)", "TKN (mg/L)", "pH", "TP (mg/L)", "OP (mg/L)", "EC (dS/m)", "TSS (mg/L)", "TDS (mg/L)")
  analyte_name <- analyte_dict[analyte_index]

  block_levels <- levels(data$block)
  treatment_levels <- levels(as.factor(data$treatment))

  block_label <- if (!is.null(block_index)) block_levels[block_index] else "All Blocks"
  treatment_label <- if (!is.null(treatment_index)) treatment_levels[treatment_index] else "All Treatments"

  sampler_labels <- names(sampler_mapping)
  n_samplers <- length(sampler_labels)
  n_dims <- length(dim(back_transformed_predictions))

  # Prepare violin + observed means
  fig <- plot_ly()
  for (s in sampler_labels) {
    s_index <- sampler_mapping[[s]]

    # Extract posterior samples
    preds <- tryCatch({
      if (n_dims == 5) {
        if (!is.null(treatment_index)) {
          back_transformed_predictions[, analyte_index, s_index, block_index, treatment_index]
        } else {
          slice <- back_transformed_predictions[, analyte_index, s_index, block_index, , drop = FALSE]
          rowMeans(matrix(slice, nrow = dim(slice)[1]), na.rm = TRUE)
        }
      } else if (n_dims == 4) {
        back_transformed_predictions[, analyte_index, s_index, block_index]
      } else if (n_dims == 3) {
        back_transformed_predictions[, analyte_index, s_index]
      } else {
        stop("Unsupported array dimensions.")
      }
    }, error = function(e) {
      warning(sprintf("Sampler %s skipped: %s", s, e$message))
      return(NULL)
    })

    if (is.null(preds)) next

    # Observed mean
    obs <- data %>% filter(A == analyte_index, S == s_index)
    if (!is.null(block_index)) obs <- obs %>% filter(block == block_levels[block_index])
    if (!is.null(treatment_index)) obs <- obs %>% filter(treatment == treatment_levels[treatment_index])
    obs_mean <- mean(obs$C_obs, na.rm = TRUE)

    # Add violin
    fig <- fig %>%
      add_trace(
        y = preds,
        type = 'violin',
        x0 = s,
        name = s,
        box = list(
          visible = TRUE,
          fillcolor = custom_colors[s],
          line = list(color = violin_edges[s], width = 2.5)
        ),
        meanline = list(visible = TRUE),
        fillcolor = custom_colors[s],
        line = list(color = violin_edges[s], width = 2.5),
        opacity = 0.7,
        points = FALSE,
        width = 0.6
      )

    # Add observed mean directly on violin
    fig <- fig %>%
      add_trace(
        y = obs_mean,
        x0 = s,
        type = 'scatter',
        mode = 'markers',
        marker = list(color = 'black', symbol = 'x', size = 10),
        name = if (s == sampler_labels[1]) "Observed Mean" else NULL,
        showlegend = (s == sampler_labels[1])
      )
  }

  # Final layout
  fig <- fig %>%
    layout(
      title = list(
        text = sprintf("Posterior Predictions vs Observed<br><sub>%s | Block: %s | Treatment: %s</sub>", analyte_name, block_label, treatment_label),
        xanchor = "center", x = 0.5
      ),
      yaxis = list(
        title = "Predicted Concentration",
        zeroline = FALSE,
        range = c(0, NULL)  # Forces y-axis to start at 0
      ),
      xaxis = list(title = "Sampler", type = "category")
    )

  return(fig)
}
```

```{r}
# Directory to save plots
dir.create("../figs/violin_predictions", showWarnings = FALSE)

# Loop over each analyte and save the violin plot
for (analyte_name in names(analyte_mapping)) {
  analyte_index <- analyte_mapping[[analyte_name]]

  # Generate the plot
  fig <- plotly_violin_predictions(
    back_transformed_predictions = back_transformed_predictions_real,
    data = d,
    sampler_mapping = sampler_mapping,
    analyte_index = analyte_index,
    block_index = 1,         # or NULL for all blocks
    treatment_index = NULL   # or specify 1, 2, etc.
  )

  # Define the output file path
  output_file <- sprintf("../figs/violin_predictions/violin_%s.png", analyte_name)

  # Save using orca (must have Orca installed)
  # Install orca support if needed
  # install.packages("processx")
  # devtools::install_github("ropensci/plotly")  # recent plotly helps
  
  # Then run once:
  # plotly::orca_available()  # should return TRUE
  #orca(fig, output_file, width = 1000, height = 800)

  cat("Saved:", output_file, "\n")
}
```

```{r}
plotly_violin_predictions(
  back_transformed_predictions = back_transformed_predictions_real,
  data = d,
  sampler_mapping = sampler_mapping,
  analyte_index = 9,     # e.g., TSS = 8
  block_index = 1,       # or NULL
  treatment_index = NULL # or 1-3
)


```

## Conclusion



